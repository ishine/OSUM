
<p align="center">
   <h1>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue</h1>
<p>

耿雪龙、邵琪杰、薛鸿飞、王水源、解晗轲、郭昭、赵懿、李国健、田文杰、王成友、赵致闲、夏康翔、张子萸、林振楠、左天伦、邵明辰、曹雨昂、马国斌、李龙豪、戴宇航、高德辉、郭大可、谢磊

<p align="center">
    <img src="../images/osum-echat/SUM.png" width="400"/>
<p>

<p align="center">
 <a href="https://www.osum-echat.npu-aslp.org/"> Test Page</a> |   <a href="https://huggingface.co/ASLP-lab/OSUM"> Ckpt</a>
<br>
📑 <a href="https://www.arxiv.org/abs/2508.09600">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href="https://aslp-lab.github.io/osum-echat.github.io/">Demo</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href="raw/fig/wechat.png">WeChat (微信)</a>&nbsp&nbsp 
</p>

共情能力对于实现口语对话系统中的自然交互至关重要，它能让机器识别年龄、性别、情绪等副语言线索，并做出恰当回应。
近年来，端到端语音语言模型（可统一语音理解与生成功能）的发展为这一领域提供了极具潜力的解决方案。
然而，当前技术仍面临诸多挑战：过度依赖大规模对话数据集、对传递共情至关重要的副语言线索提取不足，以及缺乏专门针对共情的数据集与评估框架。
为解决这些问题，我们提出了 OSUM-EChat—— 一款开源的端到端口语对话系统，旨在增强共情交互能力，尤其适用于资源受限场景。
基于[OSUM](https://github.com/ASLP-lab/OSUM/tree/main/OSUM)，OSUM-EChat 包含两大核心创新：（1）三阶段理解驱动的口语对话训练策略，该策略将大型语音理解模型的能力扩展到口语对话任务中；（2）语言 - 副语言双思维机制，通过思维链整合副语言理解与对话生成过程，使系统能够生成更具共情性的回应。这种方法在降低对大规模对话数据集依赖的同时，仍能保持高质量的共情交互。此外，我们还构建了 EChat-200K 数据集（一个丰富的共情语音到语音对话语料库）与 EChat-eval 基准（一套用于评估对话系统共情能力的综合框架）。实验结果表明，在共情响应能力方面，OSUM-EChat 优于现有的端到端口语对话模型，验证了其有效性。

<p align="center">
    <img src="../images/osum-echat/demo_cn.png" width="80%"/>
<p>


## Architecture

本节呈现 OSUM-EChat 的整体架构与核心任务概述。OSUM-EChat由3个模块组成：语音编码器（带适配器）、文本LLM、token转语音模块，并具备广泛的语音功能，包括各类语音理解任务（语音到文本)、语音合成任务、语音对话任务和文本对话任务。同时，借助内部构造的共情对话数据以及副语言信息推理机制，OSUM-EChat在语音对话任务中能够生成更具共情性的回应。


<p align="center">
    <img src="../images/osum-echat/system.png" width="80%"/>
<p>


## 训练策略
为使 OSUM-EChat 在资源受限的环境中实现共情对话，研究提出了一种名为 “理解驱动的口语对话” 的三阶段训练策略，包括理解、生成和共情三个阶段。在共情阶段，通过语言-副语言双重思维机制明确分离副语言信息和语义信息，以帮助生成更具共情的回应。

阶段 1：理解
此阶段的目标是让 LLM 理解语音中的语言和副语言信息。采用 OSUM 的 “ASR+P” 策略（其中 P 代表副语言标签，如情绪、性别、年龄和声音事件），联合训练多个 “ASR+P” 任务，仅编码器和适配器可训练。

阶段 2：生成
本阶段旨在使基于 OSUM 的理解模型具备语音生成能力，采用文本转语音（TTS）生成和语音转语音（S2S）对话两步训练过程，同时添加文本到文本（T2T）数据以保持模型的智能。

阶段 3：共情
在这个阶段，将来自语音理解的语言和副语言信息整合到对话生成过程中，显著提高模型生成上下文连贯且具有共情的回应的能力。通过语言-副语言双重思维机制，在模型生成文本和语音回应之前引入专门的思维链（CoT）过程，使模型先识别用户语音中的语言信息，再推断副语言细节，最后整合这些见解生成合适的文本和语音回应。

在思维链（CoT）的设计上，研究尝试了两种不同类型的文本形式，分别是基于标签的CoT和基于自然语言的CoT，以探究不同方式对模型共情理解与回应生成的影响。

基于标签的CoT：这种形式采用固定的模板结构进行生成。具体而言，模型会先输出自动语音识别（ASR）得到的转录文本，以此作为对用户输入的语义信息的提取；随后，会依次输出年龄、性别、语音事件、情感等预先定义好的副语言信息标签。其显著优势在于，CoT阶段生成的内容具有相对固定的格式，且长度较短，整体过程易于控制，能够高效地完成对核心副语言线索的提取与整合。不过，这种方式也存在一定局限，由于受限于预设标签的数量和范围，它无法充分表达那些未被标签涵盖的、更丰富细腻的副语言状态，例如语音中的语气强弱变化、情绪的微妙转折等。

基于自然语言的CoT：此形式摒弃了固定的标签模板，转而采用自然流畅的语言描述来完成思维链过程。模型会以连贯的文本段落，先对用户输入语音中的语义内容进行阐释（而非单纯转录），再详细描述其中蕴含的副语言信息，包括但不限于年龄特征的具体体现、性别相关的语音特质、各类语音事件的发生场景与特征，以及情感的细腻层次和变化趋势等。这种方式的优势在于能够突破标签的限制，更灵活、全面地捕捉和表达复杂的副语言状态，为后续生成更具共情性的回应提供更丰富的依据；但相对而言，其生成内容的长度和结构较难控制，可能会增加模型的计算负担，且对模型的语言组织能力提出了更高要求。

<p align="center">
    <img src="../images/osum-echat/dual_think_cn.png" width="80%"/>
<p>



## 推理结果



### 共情语音对话

在 EChat-eval 基准的评估中，OSUM-EChat 在共情对话任务上表现优异：在各类共情对话场景中，其获得的 GPT-4o 自动评分均处于高水平，尤其在多标签场景下表现突出，且对输入语音中多样声学事件的处理能力较强，具体结果如表 1 所示。
<p align="center">
    <img src="../images/osum-echat/table1.png" width="65%"/>
</p>
<p align="center"><b>表1：EChat-eval基准测试的自动评估结果。其中，“U-Driven”指的是理解驱动的口语对话训练策略，“Dual Think”指的是语言-副语言双重思维机制。</b></p>

EChat-eval 的人工评估结果进一步显示，OSUM-EChat 的综合表现优于 Qwen2.5-Omni；在情感维度的共情对话测试案例中，其性能优异，但仍逊于商业系统。值得关注的是，在其他副语言维度（如年龄、性别、声音事件等）的共情对话任务中，商业系统暂无法有效捕捉相关线索，详细数据如表 2 所示。

消融实验结果验证：将语音理解模型（OSUM）应用于口语对话任务，并结合 “语言 - 副语言双重思维机制”，可显著提升模型的共情对话能力，具体验证数据如表 1 所示。

<p align="center">
    <img src="../images/osum-echat/table3.png" width="65%"/>
</p>
<p align="center"><b>表2：代表性模型在EChat-eval基准测试上的人工评估结果。† 字节跳动的商用系统，仅由单一固定说话人提供响应。</b></p>

### 语音基础能力

OSUM-EChat 在语言智能、语音理解、语音合成三大语音基础能力的评估中，均展现出优异且稳定的性能表现，具体分析如下：

#### (1) 语言智能

依托大规模文本对话数据及研究团队内部构建的知识类语音问答数据集，OSUM-EChat 的语言智能水平与业界主流端到端语音对话模型相当，口语问答任务的具体评测结果如表 3 所示。

<p align="center">
    <img src="../images/osum-echat/table2.png" width="100%"/>
</p>
<p align="center"><b>表3：在VoiceBench基准测试中的性能表现</b></p>

#### (2) 语音理解

本研究在语音识别（ASR）、声音事件识别、情感识别、年龄识别、性别识别五大任务的开源测试集上，对 OSUM-EChat 的语音理解能力进行验证。结果表明，其性能与语音理解大模型 OSUM 基本持平（该模型LLM基座为Qwen-7B），且已接近工业级语音理解模型 Qwen2-Audio 的水平。

<p align="center">
    <img src="../images/osum-echat/table4.png" width="80%"/>
</p>
<p align="center"><b>表4：语音理解任务的性能表现</b></p>

#### (3) 语音合成

本研究在 SEED 测试集上对 OSUM-EChat 的文本转语音（TTS）能力进行评测，结果显示：其 TTS 性能优于 CosyVoice 模型，但与工业级语音对话模型及专业 TTS 模型相比仍存在差距，详细指标（词错误率、字错误率）如表 4 所示。
<p align="center">
    <img src="../images/osum-echat/table5.png" width="60%"/>
</p>
<p align="center"><b>表5： SEED 测试集上 OSUM-EChat 与近期口语对话模型的性能对比（单位：%，↓表示指标越优）</b></p>




## 如何使用OSUM-EChat的代码框架来训练和推理

### 准备环境

在开始之前请保证你的python环境已经准备好, 如下是一个建议的操作流程。我们假设你的电脑上已经安装了conda软件。如果未安装，请参考：[linux一键安装Miniconda](https://blog.csdn.net/qq_41636123/article/details/130266232) 。 我们非常建议你在linux系统的电脑上运行我们的代码。

```shell
# 新创建一个conda环境
conda create -n OSUM-EChat python=3.10
# 激活新创建的环境
conda activate OSUM-EChat
# 下载我们的代码并安装需要的python包
git clone https://github.com/ASLP-lab/OSUM.git
cd OSUM/OSUM-EChat
# 如果你在gpu训练，请先删除 requirements.txt 中torch_npu的条目，如果是npu上，则无需操作。
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 认识数据类型
本项目在基于wenet原有的raw、shard类型的基础上，设计了combine类型，本项目对三种类型均支持。

#### **raw类型**：
采用jsonl文件形式保存数据，每行一个json对象，包含如下字段：
```
{
"key": "1023390_bed51684_10", 
"txt": "你好呀姐妹，想加入专家服务系统呢，说明你很重视这个问题呀。有专业人士帮你解答，一定更安心吧？别担心，这类系统通常会有详细流程，慢慢来，一定能顺利加入的～", 
"wav": "./common_utils/fake_data/raw/wav/random.wav", 
"extra": {"age": "<ADULT>", "gender": "<FEMALE>", "think_str": "语音里能听出是一位成年女性在寻求加入孙建发专家服务系统的指引，语气中带着对专业帮助的期待；成年人在面对复杂问题时，常希望获得更权威、系统性的支持，尤其是在涉及健康或专业建议时，会更倾向于寻求专家指导。", "question": "解惑答疑请加入孙建发专家服务系统", "speech_token": [2164, 2935, 1504, 1504, 1504, 1504, 1504, 1504, 1504, 1446, 3, 1406, 1406, 1406, 1406, 3649, 3649, 1038, 15, 2162, 40, 368, 3741, 3250, 3250, 1470, 3438, 2515, 1264, 489, 2293, 351, 20, 3250, 1446, 1446, 3337, 1083, 1516, 3492, 4082, 4056, 2515, 2764, 669, 515, 109, 2646, 1865, 1117, 1117, 3011, 1406, 1406, 3649, 3649, 1038, 3636, 646, 2105, 1342, 1256, 2876, 148, 329, 2260, 1193, 890, 34, 692, 540, 73, 73, 1377, 45, 347, 50, 890, 1494, 329, 477, 3274, 1193, 661, 463, 463, 3458, 2548, 2032, 540, 962, 2844, 1854, 754, 271, 3600, 3305, 2148, 58, 2876, 1688, 3340, 1600, 1735, 3929, 186, 1446, 2571, 2664, 3062, 347, 3265, 1785, 2429, 2187, 2621, 3240, 1223, 2621, 1660, 130, 2004, 2287, 855, 3710, 1796, 60, 3768, 2472, 568, 84, 2037, 2907, 41, 569, 6, 51, 28, 130, 460, 106, 1609, 758, 2000, 3593, 3347, 3600, 2172, 40, 368, 368, 1698, 3274, 8, 1879, 31, 31, 1446, 1446, 1117, 3898, 1406, 1406, 1406, 477, 380, 501, 1317, 2569, 1705, 2058, 347, 2907, 1478, 570, 858, 1346, 1037, 6, 6, 1949, 2187, 3940, 3062, 41, 1618, 28, 2058, 2110, 266, 193, 3153, 773, 1755, 2554, 1516, 2105, 271, 1291, 3366, 3600, 1688, 1385, 2858, 2858, 1404, 1218, 1734, 1446, 1117, 26, 1736, 1289, 40, 368, 1716, 2385, 701, 2187, 1018, 4016, 101, 532, 2306, 1570, 272, 2858, 303, 3240, 646, 2844, 193, 2583, 3265, 880, 2714, 1193, 8, 272, 3305, 31, 1446, 1446, 1446, 1446, 1504, 1504, 1504, 1504, 1446, 1446, 1446, 1504, 1504, 1504, 1504, 1446, 1446, 1446, 1446, 1446, 3898, 3898, 3898, 53, 3684, 1122, 2472, 69, 1353, 2999, 2610, 1073, 570, 858, 624, 2105, 4082, 2583, 131, 1494, 1241, 966, 1446, 1504, 3682, 1615, 28, 1098, 1342, 1883, 980, 59, 624, 2105, 754, 59, 3600, 3573, 234, 2148, 802, 58, 947, 7, 3147, 2666, 234, 24, 2548, 1147, 569, 580, 20, 8, 87, 303, 768, 4006, 3437, 2841, 69, 2285, 3612, 646, 3025, 2253, 396, 8, 1879, 2208, 646, 193, 1346, 942, 38, 246, 3612, 540, 2247, 3649, 1185, 1256, 432, 1954, 1883, 1416, 966, 1504, 3898, 3898, 53, 53, 1293, 209, 3193, 740, 740, 1416, 3347, 2113, 1404, 855, 386, 744, 48, 20, 511, 109, 2209, 186, 2597, 1406, 1406, 1406, 3649, 477, 380, 70, 1289, 368, 754, 2582, 1346, 1346, 386, 1755, 532, 2306, 1755, 2664, 41, 1615, 2243, 1155, 2004, 103, 1317, 1264, 3082, 329, 1342, 1385, 1100, 1941, 1896, 760, 1377, 2306, 3600, 3710, 1256, 3582, 1564, 1446, 1446, 1446, 1446, 1446, 1446, 1446], "a_wav_path": "/home/A02_tmpdata3/osum_s2s/gender/s2s_handle_part1/data_s2s.list_wavs/1023390_bed51684_2.wav"},
"task": "<S2TCHAT> <TEXT2TOKEN> <THINK>"
}
```
具体示例可参见：
```
./common_utils/fake_data/raw/data.list
```

#### **shard类型**：
采用tar包保存数据，将若干条目的数据保存在一个tar包中，方便模型加载时一次性读取，提高读取效率。

具体示例可参见：
```
./common_utils/fake_data/shard/shards_list.txt
```

shard类型数据基于raw类型的数据得到，转换脚本为：
```
python ./common_utils/fake_data/shard/do_make_shard_from_raw.py 
```

#### **combine类型**：
shard类型便于读取，却对附加信息的修改异常困难，为此，我们设计了combine类型数据。combine类型数据是shard类型数据和raw数据类型的结合，
由tar包存储wav文件，而其他信息则存储在json文件中。

具体示例详见：
```
./common_utils/fake_data/combine/combines_list.txt # 存储每个tar包对应jsonl信息文件的路径
./common_utils/fake_data/combine/combines_tar_root.txt # 存储tar包目录
# 该类型数据的 tar 包需存储在同一目录下，即tar-dir-path与tar-file-name拼接后需能得到 tar 包的完整路径。
# 该类型数据和shard类型数据共享一个“类型标志”，即设置“shard"类型可以任意采用shard数据和combine类型数据，为区分而这，该类型数据的结果文件仅可以命名为 combines_list.txt 和 combines_tar_root.txt。
```

该类型数据需要由shard类型数据转换而来，转换脚本为：
```
python ./common_utils/fake_data/combine/do_make_combine_from_shard.py 
```

#### 数据测试
本项目提供了关于三种数据类型直接构造dataloader并读取数据的测试脚本。
测试脚本为：
```
python do_test_dataloader.py
```


### 推理
本项目提供了三种离线推理方式(已开源)和在线实时对话推理方式（即将开源).
#### 首先下载模型的checkpoint文件
```python
# Download the .pt file from Hugging Face
from huggingface_hub import hf_hub_download
# 对于自然语言think的模型
pt_file_path = hf_hub_download(repo_id="ASLP-lab/OSUM-EChat", filename="language_think_final.pt")  
# 对于基于tag think的模型
pt_file_path2 = hf_hub_download(repo_id="ASLP-lab/OSUM-EChat", filename="tag_think_final.pt")  
# token2wav模型参数，需要对其进行解压
pt_file_path3 = hf_hub_download(repo_id="ASLP-lab/OSUM-EChat", filename="CosyVoice-300M-25Hz.tar")  
# 解压token2wav模型参数，使用shell脚本的形式
import os
os.system(f"tar -xvf {pt_file_path3}")  
```

#### 基于gradio的离线推理
执行之前，先在./infer_gradio.py中设置模型参数路径
```python
CHECKPOINT_PATH_A="**/language_think_final.pt"
CHECKPOINT_PATH_B="**/tag_think_final.pt"
cosyvoice_model_path = "**/CosyVoice-300M-25Hz"
```
该脚本模型同时加载了language_think和tag_think模型，并加载了token2wav模型参数。总占现存约19G，若讲CHECKPOINT_PATH_B设置为None，则只加载language_think模型。
脚本：
```shell
python infer_gradio.py
```

#### 单条语音推理
该脚本提供了几乎所有任务类型的推理函数，包括语音理解任务，TTS任务， S2S对话，S2T对话，T2T对话等
```shell
python infer_runtime.py
```

#### 批量推理
该脚本提供了批量推理的功能，通过dataloader对形式，对数据进行批量推理。支持上述提到的三种数据类型。
```shell
bash infer_with_shards_or_raw.sh
```

#### 基于flask的在线推理
即将开放

### 训练

为确保训练流程顺利进行，请遵循以下步骤和要求：

#### 1. 数据准备

数据准备时，可采用 **raw**、 **shard** 或 **combine** 三种方式。

> **建议：** 优先采用 **shard** 方式。

数据准备完成后，必须将生成的 **数据清单（或数据索引）** 写入指定配置文件：

```yaml
OSUM-EChat/conf/data_tmp.yaml
```

#### 2. 训练启动 (Training Execution)

请运行以下主脚本启动训练任务：

```bash
OSUM-EChat/train.sh
```